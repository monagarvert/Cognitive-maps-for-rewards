{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model simulations\n",
    "Here we simulate data from our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.optimize import minimize\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from MonsterPrior import MonsterPrior\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from SuccessorRepresentation import SuccessorRepresentation\n",
    "from GraphGP import LaplacianGP\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### define main simulation function:\n",
    "\n",
    "def random_choice(options, reward_matrix, current_context):\n",
    "    \n",
    "    choice_idx = np.random.randint(0, 1)\n",
    "    choice = options[choice_idx]\n",
    "    unchosen = options[1-choice_idx]\n",
    "    reward = reward_matrix[choice, current_context -1]\n",
    "    other_reward = reward_matrix[unchosen, current_context -1]\n",
    "    regret = np.max([reward, other_reward]) - reward\n",
    "\n",
    "    return choice, choice_idx, reward, regret\n",
    "\n",
    "def directed_choice(options, reward_matrix, current_context, values):\n",
    "    \n",
    "    choice_idx = np.argmax(values)\n",
    "    choice = options[choice_idx]\n",
    "    unchosen = options[1-choice_idx]\n",
    "    reward = reward_matrix[choice, current_context -1]\n",
    "    other_reward = reward_matrix[unchosen, current_context -1]\n",
    "    regret = np.max([reward, other_reward]) - reward\n",
    "\n",
    "    return choice, choice_idx, reward, regret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def simulate_choice_data(df, reward_matrix, params, PI_dict, transition_dict, method ,optimize_l = False):\n",
    "    ''' specify which model you want to use. the function returns an array of actions,\n",
    "    rewards, regret, rpes etc.'''\n",
    "\n",
    "    last_subj = -1  # make this an id so that the first participant isn't identical to this one\n",
    "    subj_counter = -1\n",
    "\n",
    "    subj = np.array(df[\"subj\"])\n",
    "    op1 = np.array(df[\"option1\"])\n",
    "    op2 = np.array(df[\"option2\"])\n",
    "    choices = np.array(df[\"chosen_object\"])\n",
    "    contexts = np.array(df[\"map\"])\n",
    "    decisions = np.array(df[\"decision\"])\n",
    "\n",
    "    # subtract 1 from these vectors so that the monster id becomes indices\n",
    "    op1 -= 1\n",
    "    op2 -= 1\n",
    "    choices -= 1\n",
    "    decisions -= 1\n",
    "    states = np.arange(0, 12)\n",
    "    ## set hyperparameters\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    lengthscale = params[\"lengthscale\"]\n",
    "    sr_diffusion = 1\n",
    "    \n",
    "    choices = np.zeros(len(subj))\n",
    "    choice_indices = np.zeros(len(subj))\n",
    "    gains = np.zeros(len(subj))\n",
    "    regret = np.zeros(len(subj))\n",
    "    RPE = np.zeros(len(subj))\n",
    "    \n",
    "    for i, subj_id in enumerate(subj):\n",
    "        current_context = contexts[i]\n",
    "        if subj_id != last_subj:\n",
    "\n",
    "            subj_counter += 1\n",
    "            trial_counter = 0\n",
    "            loc = PI_dict[subj_id]\n",
    "\n",
    "            context_dict = {}\n",
    "            context_dict[1] ={\"training_idx\": [], \"rewards\": [], \"state_rewards\" : np.zeros(len(np.arange(12)))} #copy.deepcopy(dict_template)\n",
    "            context_dict[2] = {\"training_idx\": [], \"rewards\": [], \"state_rewards\" : np.zeros(len(np.arange(12)))} #copy.deepcopy(dict_template)\n",
    "\n",
    "            ### SR\n",
    "\n",
    "            if method == \"SR-GP\" or method == \"Compositional\":\n",
    "                seq_list = []\n",
    "                for run, seq in transition_dict[subj_id].items():\n",
    "                    seq_ = copy.deepcopy(seq)\n",
    "                    seq_ -=1\n",
    "                    seq_list.append(seq_)\n",
    "\n",
    "                    sr_model = SuccessorRepresentation(states, seq_list, alpha=learning_rate)\n",
    "                    SR = sr_model.get_SR()\n",
    "\n",
    "\n",
    "                    SRL = estimate_laplacian(SR, gamma = sr_model.gamma, subj_id = subj_id, plot=False)\n",
    "                    SR_kernel = scipy.linalg.expm(-sr_diffusion*SRL)\n",
    "\n",
    "        \n",
    "            if method == \"Euclidean\" or method == \"Compositional\":            \n",
    "                estimated_euclidean_kernel = RBF(loc, loc, l=lengthscale)\n",
    "\n",
    "            if method == \"Compositional\":\n",
    "                comp_kernel = (estimated_euclidean_kernel + SR_kernel)/2\n",
    "\n",
    "\n",
    "            ### add observations for this context\n",
    "            options = [op1[i], op2[i]]\n",
    "            choice, c_idx, reward, regret_i = random_choice(options, reward_matrix, current_context)\n",
    "            \n",
    "            choices[i] = choice\n",
    "            choice_indices[i] = c_idx            \n",
    "            gains[i] = reward\n",
    "            regret[i] = regret_i\n",
    "            RPE[i] =  - reward            \n",
    "\n",
    "            context_dict[current_context][\"training_idx\"].append(choice)\n",
    "            context_dict[current_context][\"rewards\"].append(reward)\n",
    "\n",
    "            \n",
    "            ## set the last subj_id to the current one\n",
    "            last_subj = subj_id\n",
    "            trial_counter += 1\n",
    "\n",
    "\n",
    "        elif len(context_dict[current_context][\"rewards\"]) == 0:  # check if participant has been able to make any observations in this context yet \n",
    "            # if not then let choice be random, and store observations into context dict\n",
    "\n",
    "            options = [op1[i], op2[i]]\n",
    "            choice, c_idx, reward, regret_i = random_choice(options, reward_matrix, current_context)\n",
    "\n",
    "            choices[i] = choice\n",
    "            choice_indices[i] = c_idx            \n",
    "            gains[i] = reward\n",
    "            regret[i] = regret_i\n",
    "            RPE[i] =  - reward            \n",
    "\n",
    "            \n",
    "            context_dict[current_context][\"training_idx\"].append(choice)\n",
    "            context_dict[current_context][\"rewards\"].append(reward)\n",
    "                        \n",
    "            trial_counter += 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            options = [op1[i], op2[i]]\n",
    "            training_idx = context_dict[current_context][\"training_idx\"] # the training indices for the gps\n",
    "            y = np.array(copy.copy(context_dict[current_context][\"rewards\"]))  # for use in the gp models. we copy this so we can normalize it and convert it into an array without messing with the original set of reward observations\n",
    "\n",
    "            y_prime = np.append(y, reward)\n",
    "\n",
    "            if y.std() != 0:\n",
    "                y = (y- y.mean())/y.std()\n",
    "                y_prime = (y_prime - y_prime.mean())/y_prime.std()\n",
    "\n",
    "            else:\n",
    "                y = (y - y.mean())\n",
    "                y_prime = (y_prime - y_prime.mean())\n",
    "\n",
    "            reward_normalized = y_prime[-1]\n",
    "            ### Euclidean prediction error\n",
    "            if method == \"SR-GP\":\n",
    "                if optimize_l:\n",
    "                    preds = optimize_diffusion_gp(SRL, training_idx, y, option_indices = options)\n",
    "                else:\n",
    "                    preds = estimate_GP(SR_kernel, y, training_idx, option_indices=options)\n",
    "            elif method == \"Euclidean\":\n",
    "                if optimize_l:\n",
    "                    preds = optimize_gp(loc, training_idx, y, option_indices = options)\n",
    "                else:\n",
    "                                    \n",
    "                    preds = estimate_GP(estimated_euclidean_kernel, y, training_idx, option_indices=options)\n",
    "\n",
    "            elif method == \"Mean-tracker\":\n",
    "                BMT_kernel = np.eye(12)\n",
    "                preds = estimate_GP(BMT_kernel, y, training_idx, option_indices=options)\n",
    "            else:                \n",
    "                preds = estimate_GP(comp_kernel, y, training_idx, option_indices=options)\n",
    "                \n",
    "            ###\n",
    "            choice, c_idx, reward, regret_i = directed_choice(options, reward_matrix, current_context, preds)\n",
    "            \n",
    "            choices[i] = choice\n",
    "            choice_indices[i] = c_idx\n",
    "            gains[i] = reward\n",
    "            regret[i] = regret_i\n",
    "            RPE[i] = preds[c_idx] - reward_normalized\n",
    "\n",
    "            ### update arrays:\n",
    "            context_dict[current_context][\"training_idx\"].append(choice)\n",
    "            context_dict[current_context][\"rewards\"].append(reward)\n",
    "            context_dict[current_context][\"state_rewards\"][choice] = reward\n",
    "\n",
    "            trial_counter += 1\n",
    "\n",
    "    results = np.zeros((len(subj), 5))\n",
    "    results[:, 0] = choices\n",
    "    results[:, 1] = choice_indices\n",
    "    results[:, 2] = gains\n",
    "    results[:, 3] = regret\n",
    "    results[:, 4] = RPE\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Open pickled files\n",
    "with open('occupancy_counts.pickle', 'rb') as handle:\n",
    "    occupancy_dict = pickle.load(handle)\n",
    "\n",
    "with open('transitions.pickle', 'rb') as handle:\n",
    "    transition_dict = pickle.load(handle)\n",
    "\n",
    "with open('subjective_kernel.pickle', 'rb') as handle:\n",
    "    subjective_kernel_dict = pickle.load(handle)\n",
    "\n",
    "with open('subjective_grid_search_dict.pickle', 'rb') as handle:\n",
    "    subj_kernel_grid_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "with open('path_integration_kernels.pickle', 'rb') as handle:\n",
    "    estimated_euclidean_kernels = pickle.load(handle)\n",
    "\n",
    "with open('path_integration_monster_locations.pickle', 'rb') as handle:\n",
    "    PI_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "### Unpack choice data and create reward matrices\n",
    "\n",
    "df = pd.read_csv('choice_data.csv')\n",
    "r_df = pd.read_csv(\"moster_rewards.csv\")\n",
    "reward_matrix = np.zeros((12, 2))\n",
    "reward_matrix[:, 0] = r_df[\"ctx1\"]\n",
    "reward_matrix[:, 1] = r_df[\"ctx2\"]\n",
    "\n",
    "\n",
    "params = {\"lengthscale\": 1.24, \"learning_rate\": 0.4125}\n",
    "params_comp = {\"lengthscale\": 2.05, \"learning_rate\": 0.01}\n",
    "header = [\"choices\", \"choice_indices\", \"rewards\", \"regret\", \"RPE\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n",
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tankred\\anaconda3\\anacondadistributor\\envs\\tensor\\lib\\site-packages\\numpy\\lib\\index_tricks.py:876: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  a.flat[:end:step] = val\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n",
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n",
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n",
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n",
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n",
      "Warning: Cholesky didn't work - trying to remove negative eigenvalues and reconstruct using Eigendecomposition\n",
      "Could not compute cholesky - lengthscale is set to 1\n"
     ]
    }
   ],
   "source": [
    "################# Run simulations\n",
    "#################################\n",
    "\n",
    "# ########EUCLIDEAN:\n",
    "# ##################\n",
    "\n",
    "euc_sim = simulate_choice_data(df, reward_matrix, params, PI_dict, transition_dict, method=\"Euclidean\", optimize_l = False)\n",
    "euc_sim.to_csv(\"model_simulations/simulations/euclidean_sim.csv\", index=False, header=header)\n",
    "\n",
    "\n",
    "\n",
    "########SR-GP:\n",
    "##############\n",
    "\n",
    "sr_gp_sim = simulate_choice_data(df, reward_matrix, params, PI_dict, transition_dict, method=\"SR-GP\", optimize_l = False)\n",
    "sr_gp_sim.to_csv(\"model_simulations/simulations/sr_gp_sim.csv\", index=False, header=header)\n",
    "\n",
    "########SR-GP-OPTIMIZED:\n",
    "########################\n",
    "\n",
    "sr_gp_sim_optimized = simulate_choice_data(df, reward_matrix, params, PI_dict, transition_dict, method=\"SR-GP\", optimize_l = True)\n",
    "sr_gp_sim_optimized.to_csv(\"model_simulations/simulations/sr_gp_sim_optimized.csv\", index=False, header=header)\n",
    "\n",
    "########COMPOSITIONAL:\n",
    "######################\n",
    "\n",
    "compositional_sim = simulate_choice_data(df, reward_matrix, params_comp, PI_dict, transition_dict, method=\"Compositional\", optimize_l = False)\n",
    "compositional_sim.to_csv(\"model_simulations/simulations/compositional_sim.csv\", index=False, header=header)\n",
    "\n",
    "########MEAN-TRACKER:\n",
    "#####################\n",
    "\n",
    "mean_tracker_sim = simulate_choice_data(df, reward_matrix, params, PI_dict, transition_dict, method=\"Mean-tracker\", optimize_l = False)\n",
    "mean_tracker_sim.to_csv(\"model_simulations/mean_tracker_sim.csv\", index=False, header = header)\n",
    "\n",
    "\n",
    "########EUCLIDEAN OPTIMIZED:\n",
    "############################\n",
    "\n",
    "euc_sim = simulate_choice_data(df, reward_matrix, params, PI_dict, transition_dict, method=\"Euclidean\", optimize_l = True)\n",
    "euc_sim.to_csv(\"model_simulations/simulations/euclidean_sim_optimized.csv\", index=False, header=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
