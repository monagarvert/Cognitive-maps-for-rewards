{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "\n",
    "Here we validate our models on the simulated data they generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from MonsterPrior import MonsterPrior\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from SuccessorRepresentation import SuccessorRepresentation\n",
    "from GraphGP import LaplacianGP\n",
    "import copy\n",
    "import time\n",
    "from utils import *\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this script I validate each model on the data generated by every model we consider\n",
    "\n",
    "def add_diff(matrix, predictions, trial):\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        diff = pred[0] - pred[1]\n",
    "        matrix[trial, i] = diff\n",
    "\n",
    "def validate_model_sims(choice_data, simulation, simulation_name, params):\n",
    "\n",
    "    \"\"\" Here we make each model go through a dataset generated by another model and generate predictions on monster values based on the rewards and decisions made by that model. The goal is to see if we can recover the ground truth models in the first place\"\"\"\n",
    "    \n",
    "    op1 = np.array(choice_data[\"option1\"])\n",
    "    op2 = np.array(choice_data[\"option2\"])\n",
    "    op1 -= 1\n",
    "    op2 -= 1\n",
    "    contexts = np.array(choice_data[\"map\"])\n",
    "    subj = np.array(choice_data[\"subj\"])\n",
    "    last_subj = -1\n",
    "    subj_counter = -1\n",
    "    choices = np.array(simulation[\"choices\"], dtype=int)\n",
    "    decisions = np.array(simulation[\"choice_indices\"])\n",
    "    rewards = np.array(simulation[\"rewards\"])\n",
    "    states = np.arange(12)\n",
    "    sr_diffusion = 1\n",
    "    \n",
    "    header = [\"SR-GP\", \"Euclidean\", \"Compositional\", \"Mean-tracker\", \"Optimized-SR-GP\"]\n",
    "    predicted_differences = np.zeros((len(subj), len(header)))\n",
    "\n",
    "    lengthscale = params[\"lengthscale\"]\n",
    "    comp_lengthscale = params[\"compositional_lengthscale\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    comp_lr = params[\"compositional_lr\"]\n",
    "    \n",
    "    for i, subj_id in enumerate(subj):\n",
    "        current_context = contexts[i]\n",
    "        if subj_id != last_subj:\n",
    "\n",
    "\n",
    "            subj_counter += 1\n",
    "            trial_counter = 0\n",
    "\n",
    "            loc = PI_dict[subj_id]\n",
    "\n",
    "            context_dict = {}\n",
    "            context_dict[1] ={\"training_idx\": [], \"rewards\": [], \"state_rewards\" : np.zeros(len(np.arange(12)))} #copy.deepcopy(dict_template)\n",
    "            context_dict[2] = {\"training_idx\": [], \"rewards\": [], \"state_rewards\" : np.zeros(len(np.arange(12)))} #copy.deepcopy(dict_template)\n",
    "\n",
    "            ### SR\n",
    "\n",
    "            seq_list = []\n",
    "            for run, seq in transition_dict[subj_id].items():\n",
    "                seq_ = copy.deepcopy(seq)\n",
    "                seq_ -=1\n",
    "                seq_list.append(seq_)\n",
    "\n",
    "            sr_model = SuccessorRepresentation(states, seq_list, alpha=learning_rate)\n",
    "            SR = sr_model.get_SR()\n",
    "\n",
    "            SRL = estimate_laplacian(SR, gamma = sr_model.gamma, subj_id = subj_id, plot=False)\n",
    "\n",
    "            SR_kernel = scipy.linalg.expm(-sr_diffusion*SRL)\n",
    "            ## sr for the compositional kernel:\n",
    "            sr_model = SuccessorRepresentation(states, seq_list, alpha=comp_lr)\n",
    "            SR = sr_model.get_SR()\n",
    "\n",
    "            SRL = estimate_laplacian(SR, gamma = sr_model.gamma, subj_id = subj_id, plot=False)\n",
    "            SR_kernel_comp = scipy.linalg.expm(-sr_diffusion*SRL)\n",
    "            ### Euclidean\n",
    "\n",
    "            estimated_euclidean_kernel = RBF(loc, loc, l=lengthscale)\n",
    "            estimated_euclidean_kernel_comp = RBF(loc, loc, l=comp_lengthscale)\n",
    "\n",
    "            comp_kernel = (estimated_euclidean_kernel_comp + SR_kernel_comp)/2\n",
    "\n",
    "            ### add observations for this context\n",
    "            options = [op1[i], op2[i]]\n",
    "            choice = choices[i]\n",
    "            reward = rewards[i]\n",
    "\n",
    "\n",
    "            context_dict[current_context][\"training_idx\"].append(choice)\n",
    "            context_dict[current_context][\"rewards\"].append(reward)\n",
    "\n",
    "\n",
    "\n",
    "            ## set the last subj_id to the current one\n",
    "            last_subj = subj_id\n",
    "            trial_counter += 1\n",
    "\n",
    "\n",
    "        elif len(context_dict[current_context][\"rewards\"]) == 0:  # check if participant has been able to make any observations in this context yet \n",
    "            # if not then let choice be random, and store observations into context dict\n",
    "\n",
    "            options = [op1[i], op2[i]]\n",
    "            choice = choices[i]\n",
    "            reward = rewards[i]\n",
    "\n",
    "            context_dict[current_context][\"training_idx\"].append(choice)\n",
    "            context_dict[current_context][\"rewards\"].append(reward)\n",
    "\n",
    "            trial_counter += 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            options = [op1[i], op2[i]]\n",
    "\n",
    "            choice = choices[i]\n",
    "            decision = decisions[i]\n",
    "            unchosen = 1 - decision\n",
    "            reward = rewards[i]\n",
    "\n",
    "            training_idx = context_dict[current_context][\"training_idx\"] # the training indices for the gps\n",
    "            R = copy.copy(context_dict[current_context][\"state_rewards\"]) # an array with rewards for each state for the SR. We copy so that it doesn't change when we normalize it\n",
    "            y = np.array(copy.copy(context_dict[current_context][\"rewards\"]))  # for use in the gp models. we copy this so we can normalize it and convert it into an array without messing with the original set of reward observations\n",
    "            ## we don't normalize now\n",
    "\n",
    "            y_prime = np.append(y, reward)\n",
    "\n",
    "            if y.std() != 0:\n",
    "                y = (y- y.mean())/y.std()\n",
    "                y_prime = (y_prime - y_prime.mean())/y_prime.std()\n",
    "\n",
    "            else:\n",
    "                y = (y - y.mean())\n",
    "                y_prime = (y_prime - y_prime.mean())\n",
    "\n",
    "    \n",
    "            ### Euclidean prediction error\n",
    "            SR_GP_preds = estimate_GP(SR_kernel, y, training_idx, option_indices=options)\n",
    "            euclidean_preds = estimate_GP(estimated_euclidean_kernel, y, training_idx, option_indices=options)\n",
    "            comp_preds = estimate_GP(comp_kernel, y, training_idx, option_indices=options)\n",
    "            MT_preds = estimate_GP(np.eye(12), y, training_idx, option_indices=options)\n",
    "            optimized_sr_gp_preds = optimize_diffusion_gp(SRL, training_idx, y, option_indices = options)\n",
    "\n",
    "            model_predictions = [SR_GP_preds, euclidean_preds, comp_preds, MT_preds, optimized_sr_gp_preds]\n",
    "\n",
    "\n",
    "\n",
    "            add_diff(matrix=predicted_differences, predictions=model_predictions, trial=i)\n",
    "            \n",
    "\n",
    "            ### update arrays:\n",
    "            context_dict[current_context][\"training_idx\"].append(choice)\n",
    "            context_dict[current_context][\"rewards\"].append(reward)\n",
    "\n",
    "\n",
    "            trial_counter += 1\n",
    "\n",
    "    diff_df = pd.DataFrame(predicted_differences)\n",
    "    diff_df.to_csv(f\"model_simulations/validations/{simulation_name}-validation.csv\", index=False, header = header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load simulated data\n",
    "\n",
    "choice_data = pd.read_csv(\"choice_data.csv\")\n",
    "folder = \"model_simulations/simulations\"\n",
    "sr_gp_sim = pd.read_csv(f\"{folder}/sr_gp_sim.csv\")\n",
    "euc_sim = pd.read_csv(f\"{folder}/euclidean_sim.csv\")\n",
    "comp_sim = pd.read_csv(f\"{folder}/compositional_sim.csv\")\n",
    "mt_sim =  pd.read_csv(f\"{folder}/mean_tracker_sim.csv\")\n",
    "sr_gp_opt_sim =  pd.read_csv(f\"{folder}/sr_gp_sim_optimized.csv\")\n",
    "\n",
    "### open pickled data\n",
    "with open('occupancy_counts.pickle', 'rb') as handle:\n",
    "    occupancy_dict = pickle.load(handle)\n",
    "\n",
    "with open('transitions.pickle', 'rb') as handle:\n",
    "    transition_dict = pickle.load(handle)\n",
    "\n",
    "with open('subjective_kernel.pickle', 'rb') as handle:\n",
    "    subjective_kernel_dict = pickle.load(handle)\n",
    "\n",
    "with open('subjective_grid_search_dict.pickle', 'rb') as handle:\n",
    "    subj_kernel_grid_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "with open('path_integration_kernels.pickle', 'rb') as handle:\n",
    "    estimated_euclidean_kernels = pickle.load(handle)\n",
    "\n",
    "with open('path_integration_monster_locations.pickle', 'rb') as handle:\n",
    "    PI_dict = pickle.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run models on simulated data\n",
    "\n",
    "params = {\"lengthscale\": 1.24, \"learning_rate\": 0.4125, \"compositional_lengthscale\": 2.05, \"compositional_lr\": 0.01}\n",
    "simulation_datasets = [sr_gp_sim, euc_sim, comp_sim, mt_sim, sr_gp_opt_sim]\n",
    "sim_names = [\"SR-GP\", \"Euclidean\", \"Compositional\", \"Mean-tracker\", \"SR-GP-optimized\"]\n",
    "\n",
    "\n",
    "\n",
    "for (dataset, name) in zip(simulation_datasets, sim_names):\n",
    "    validate_model_sims(choice_data, simulation=dataset, simulation_name=name, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
