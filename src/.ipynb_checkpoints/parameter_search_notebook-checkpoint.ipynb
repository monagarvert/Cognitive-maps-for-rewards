{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictors for choice task with various hyper-parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from MonsterPrior import MonsterPrior\n",
    "from importlib import reload\n",
    "from SuccessorRepresentation import SuccessorRepresentation\n",
    "from GraphGP import LaplacianGP\n",
    "import copy\n",
    "import time\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "### This is the file where we generate all the\n",
    "### model predictors with the various parameter\n",
    "### settings that we wish to optimize.\n",
    "### The predictors can be generated seperately.\n",
    "### If the whole script is run, all datasets\n",
    "### are generated (this takes a while)\n",
    "#################################################\n",
    "#################################################\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "### Open pickled files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('transitions.pickle', 'rb') as handle:\n",
    "    transition_dict = pickle.load(handle)\n",
    "\n",
    "\n",
    "with open('path_integration_kernels.pickle', 'rb') as handle:\n",
    "    estimated_euclidean_kernels = pickle.load(handle)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Preamble - set some variables, create dataframes etc, nothing crazy\n",
    "df = pd.read_csv('choice_data.csv')\n",
    "\n",
    "\n",
    "subj = np.array(df[\"subj\"])\n",
    "op1 = np.array(df[\"option1\"])\n",
    "op2 = np.array(df[\"option2\"])\n",
    "choices = np.array(df[\"chosen_object\"])\n",
    "contexts = np.array(df[\"map\"])\n",
    "decisions = np.array(df[\"decision\"])\n",
    "\n",
    "# subtract 1 from these vectors so that the monster id becomes indices\n",
    "op1 -= 1\n",
    "op2 -= 1\n",
    "choices -= 1\n",
    "decisions -= 1\n",
    "\n",
    "rewards = np.array(df[\"chosen_value\"])\n",
    "\n",
    "states = np.arange(0, 12)\n",
    "\n",
    "###############################################\n",
    "### define the model value estimation functions, these will all be called for each data point\n",
    "###############################################\n",
    "\n",
    "def estimate_successor_model(SR, R, option_indices):\n",
    "    \n",
    "    V = SR @ R\n",
    "    V_i = V[option_indices]\n",
    "    return V_i[0] - V_i[1]\n",
    "\n",
    "def estimate_euclidean_model(K, R, training_idx, option_indices):\n",
    "    \n",
    "    gp = LaplacianGP()\n",
    "    gp.set_training_data(training_idx, R)\n",
    "    gp.set_covariance(K)\n",
    "    mu = gp.mean()\n",
    "    options = mu[option_indices]\n",
    "    return options[0] - options[1]\n",
    "\n",
    "def estimate_laplacian(M, gamma, lmbd=0.000001):\n",
    "    T = estimate_transition_matrix(M, gamma)\n",
    "    # convert transition matrix to normalized laplacian\n",
    "    np.fill_diagonal(T, 0)\n",
    "    T[T<0] = 0  # remove negative entries\n",
    "    \n",
    "    ## make matrices symmetric again!\n",
    "    T_upper = np.triu(T)\n",
    "    T_lower = np.tril(T)\n",
    "\n",
    "    T_upperT = T_upper.T\n",
    "    T_lowerT = T_lower.T\n",
    "\n",
    "    T_upper = np.maximum(T_upper, T_lowerT)\n",
    "    T_lower = np.maximum(T_upperT, T_lower)\n",
    "    T = T_upper + T_lower\n",
    "    ###\n",
    "    \n",
    "    L = np.eye(len(T)) - T\n",
    " \n",
    "    return L\n",
    "    \n",
    "\n",
    "\n",
    "def weigh_kernels(k1, k2, training_idx, y):\n",
    "    gp1 = LaplacianGP()\n",
    "    gp1.set_training_data(training_idx, y)\n",
    "    gp1.set_covariance(k1)\n",
    "    nll1 = gp1.evaluate_nll()\n",
    "    \n",
    "    gp2 = LaplacianGP()\n",
    "    gp2.set_training_data(training_idx, y)\n",
    "    gp2.set_covariance(k2)\n",
    "    nll2 = gp2.evaluate_nll()\n",
    "    \n",
    "    k1_ml = np.exp(-nll1)\n",
    "    k2_ml = np.exp(-nll2)\n",
    "\n",
    "    p1 = k1_ml / (k1_ml + k2_ml)\n",
    "    p2 = 1-p1\n",
    "    return p1, p2\n",
    "\n",
    "    \n",
    "\n",
    "def estimate_GP_full(K, R, training_idx):\n",
    "    gp = LaplacianGP()\n",
    "    gp.set_training_data(training_idx, R)\n",
    "    gp.set_covariance(K)\n",
    "    mu = gp.mean()\n",
    "    return mu\n",
    "\n",
    "def estimate_GP(K, R, training_idx, option_indices):\n",
    "    gp = LaplacianGP()\n",
    "    gp.set_training_data(training_idx, R)\n",
    "    gp.set_covariance(K)\n",
    "    mu = gp.mean()\n",
    "    options = mu[option_indices]\n",
    "    return options[0] - options[1]\n",
    "    \n",
    "\n",
    "\n",
    "def estimate_transition_matrix(M, gamma, lmbd = 0.0000001):\n",
    "\n",
    "    I = np.eye(len(M))\n",
    "    jitter = lmbd * np.eye(len(M))\n",
    "\n",
    "    T = (np.linalg.inv(M + jitter) - I) / -gamma\n",
    "    return T\n",
    "\n",
    "def estimate_sr_graph_model(sr_graph, R, training_idx, option_indices, lengthscale):\n",
    "    \n",
    "    gp = LaplacianGP()\n",
    "    gp.train(sr_graph, training_idx, R, alpha=lengthscale)\n",
    "\n",
    "    mu = gp.mean()\n",
    "    options = mu[option_indices]\n",
    "    return options[0] - options[1]\n",
    "\n",
    "\n",
    "def optimize_diffusion_gp(L, training_idx, y, option_indices):\n",
    "    gp = LaplacianGP()\n",
    "    gp.set_training_data(training_idx, y)\n",
    "    gp.set_laplacian_matrix(L)\n",
    "\n",
    "    lengthscale = gp.minimize_nll_diffusion()\n",
    "    K_optimal = scipy.linalg.exp(-lengthscale*L)\n",
    "\n",
    "    gp.set_covariance(K_optimal)\n",
    "    mu = gp.mean()\n",
    "    options = mu[option_indices]\n",
    "    return options[0] - options[1]    \n",
    "\n",
    "def optimize_gp(X, training_idx, y, option_indices):\n",
    "    gp = LaplacianGP()\n",
    "    gp.set_training_data(training_idx, y)\n",
    "    X_train = X[training_idx]\n",
    "    K_optimal, l, n = gp.minimize_nll(X, X_train)\n",
    "\n",
    "    gp.set_covariance(K_optimal)\n",
    "    mu = gp.mean()\n",
    "    options = mu[option_indices]\n",
    "    return options[0] - options[1]    \n",
    "\n",
    "def estimate_graph_model(graph, R, training_idx, option_indices, lengthscale):\n",
    "\n",
    "    gp = LaplacianGP()\n",
    "    gp.train(graph, training_idx, R, alpha=lengthscale)\n",
    "    mu = gp.mean()\n",
    "\n",
    "    options = mu[option_indices]\n",
    "    return options[0] - options[1]\n",
    "\n",
    "def RBF(X1, X2, var = 1, l = 1):\n",
    "        \n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    return var**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "def make_symmetric(T):\n",
    "    T_upper = np.triu(T)\n",
    "    T_lower = np.tril(T)\n",
    "\n",
    "    T_upperT = T_upper.T\n",
    "    T_lowerT = T_lower.T\n",
    "\n",
    "    T_upper = np.maximum(T_upper, T_lowerT)\n",
    "    T_lower = np.maximum(T_upperT, T_lower)\n",
    "    T = T_upper + T_lower\n",
    "    return T\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def SR_softmax(prior_T, rewards):\n",
    "    nodes = np.arange(len(rewards))\n",
    "    T = np.zeros((len(nodes),len(nodes)))\n",
    "    for i, node in enumerate(nodes):\n",
    "        p = np.zeros(len(nodes))\n",
    "        adj = prior_T[i]\n",
    "        adj = np.where(adj > 0)[0]#adj[adj>0]\n",
    "\n",
    "\n",
    "        r_adj = rewards[adj]\n",
    "        s_max = softmax(r_adj)\n",
    "        p[adj] = s_max\n",
    "        T[i] = p\n",
    "\n",
    "    T = make_symmetric(T)\n",
    "    L = np.eye(len(nodes)) - T\n",
    "    return L, T\n",
    "    \n",
    "\n",
    "def SR_bayesian(prior_T, rewards):\n",
    "    nodes = np.arange(len(rewards))\n",
    "    T = np.zeros((len(nodes),len(nodes)))\n",
    "    for i, node in enumerate(nodes):\n",
    "        p = np.zeros(len(nodes))\n",
    "        adj = prior_T[i]\n",
    "        adj = np.where(adj > 0)[0]#adj[adj>0]\n",
    "        r_adj = rewards[adj]\n",
    "        s_max = softmax(r_adj)\n",
    "        prior = prior_T[i]\n",
    "        prior /= np.sum(prior)\n",
    "        p[adj] = s_max\n",
    "        p = (p*prior)/np.sum(p*prior)\n",
    "        T[i] = p\n",
    "\n",
    "    T = make_symmetric(T)\n",
    "    L = np.eye(len(nodes)) - T\n",
    "    return L, T\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "######################\n",
    "### Create analysis loop\n",
    "######################\n",
    "\n",
    "### model possibilities\n",
    "## Euclidean\n",
    "## Temporal\n",
    "## SR\n",
    "## Compositional\n",
    "## Optimized Euclidean\n",
    "## SR softmax\n",
    "\n",
    "\n",
    "def param_search(params, model, header_is_params, file_name):\n",
    "    '''\n",
    "    Function for searching through a set of parameters and saving the estimated value\n",
    "    difference for each choice (for each parameter setting) in a csv file.\n",
    "\n",
    "    params: numpy array with the parameter settings\n",
    "    model: A string specifying model type. Must be:\n",
    "    - \"Euclidean\"\n",
    "    - \"Temporal\"\n",
    "    - \"SR\"\n",
    "    - \"Compositional\"\n",
    "    - \"Euclidean-optimized\"\n",
    "    - \"SR-softmax\"\n",
    "    header_is_params: Boolean specifying whether the parameter values should be used as header\n",
    "    file_name: a string specifying the file name\n",
    "    '''\n",
    "\n",
    "    \n",
    "    num_samples = len(params)\n",
    "    progress_counter = 0\n",
    "\n",
    "    SR_based = [\"Temporal\", \"SR\", \"Compositional\", \"SR-softmax\"]\n",
    "    euclidean_based = [\"Euclidean\", \"Compositional\",\"Euclidean-optimized\", \"SR-softmax\"]\n",
    "\n",
    "    \n",
    "    ### this won't be optimized\n",
    "    sr_diffusion = 1\n",
    "\n",
    "    ## configure data saving\n",
    "    if header_is_params:\n",
    "        \n",
    "        header = params\n",
    "        if model == \"Euclidean\":\n",
    "            header = np.linspace(0.1, 4, 100)[params]\n",
    "    else:\n",
    "        header = np.arange(len(params))\n",
    "        \n",
    "    data_frame = np.zeros((len(subj), num_samples))\n",
    "\n",
    "\n",
    "    ## start loop\n",
    "    \n",
    "    for n in range(num_samples):\n",
    "\n",
    "        print(\"progress %: \", progress_counter / num_samples)\n",
    "        progress_counter += 1\n",
    "        params_i = params[n]\n",
    "\n",
    "        if model==\"Compositional\" or model == \"SR-softmax\":\n",
    "            ## if model is compositional, unpack the values from 2darray\n",
    "            learning_rate = params[n, 0]\n",
    "            lengthscale_index = int(params[n, 1])\n",
    "            estimated_euclidean = estimated_euclidean_kernels[lengthscale_index]\n",
    "            \n",
    "\n",
    "\n",
    "        elif model in SR_based:\n",
    "            learning_rate = params_i  # in case model is SR based\n",
    "        elif model in euclidean_based:\n",
    "            mp = MonsterPrior(np.linspace(0.1, 4, 100)[params_i])\n",
    "            estimated_euclidean = mp.get_kernel_matrix()\n",
    "            monster_loc = mp.pos\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        last_subj = -1\n",
    "        for i, subj_id in enumerate(subj):\n",
    "\n",
    "            current_context = contexts[i]\n",
    "            if subj_id != last_subj:\n",
    "\n",
    "                ### If there's a change in the subject, recompute all the\n",
    "                ### representations, based on the exploration trials of the subject whose behaviour we seek to model.\n",
    "\n",
    "                context_dict = {}\n",
    "                context_dict[1] ={\"training_idx\": [], \"rewards\": [], \"state_rewards\" : np.zeros(len(np.arange(12)))} \n",
    "                context_dict[2] = {\"training_idx\": [], \"rewards\": [], \"state_rewards\" : np.zeros(len(np.arange(12)))} \n",
    "\n",
    "\n",
    "\n",
    "                ### SR\n",
    "                if model in SR_based:\n",
    "                    seq_list = []\n",
    "                    for run, seq in transition_dict[subj_id].items():\n",
    "                        seq_ = copy.deepcopy(seq)\n",
    "                        seq_ -=1\n",
    "                        seq_list.append(seq_)\n",
    "\n",
    "\n",
    "                    sr_model = SuccessorRepresentation(states, seq_list, alpha=learning_rate)\n",
    "                    SR = sr_model.get_SR()\n",
    "\n",
    "                    SRL = estimate_laplacian(SR, gamma = sr_model.gamma)\n",
    "                    SR_K = scipy.linalg.expm(-sr_diffusion*SRL)\n",
    "\n",
    "                if model == \"Euclidean\":\n",
    "\n",
    "                    estimated_euclidean_K = estimated_euclidean\n",
    "\n",
    "\n",
    "                if model == \"SR-softmax\":\n",
    "                    SR_dict = {}\n",
    "                    SR_dict[1] = {}\n",
    "                    SR_dict[2] = {}\n",
    "\n",
    "                    T_prior = estimate_transition_matrix(SR, gamma=sr_model.gamma)\n",
    "                    SR_dict[1][\"kernel\"] = SR_K\n",
    "                    SR_dict[2][\"kernel\"] = SR_K\n",
    "                    SR_dict[1][\"T\"] = T_prior\n",
    "                    SR_dict[2][\"T\"] = T_prior\n",
    "\n",
    "                \n",
    "                if model == \"Compositional\" or model == \"SR-softmax\":\n",
    "                    estimated_euclidean_K = estimated_euclidean[subj_id]                    \n",
    "                    compositional_kernel = (SR_K + estimated_euclidean_K)/2\n",
    "\n",
    "                ### add observations for this context\n",
    "                options = [op1[i], op2[i]]\n",
    "                choice = choices[i]\n",
    "                reward = rewards[i]\n",
    "\n",
    "                context_dict[current_context][\"training_idx\"].append(choice)\n",
    "                context_dict[current_context][\"rewards\"].append(reward)\n",
    "                context_dict[current_context][\"state_rewards\"][choice] = reward\n",
    "\n",
    "                ## set the last subj to the current one\n",
    "                last_subj = subj_id\n",
    "\n",
    "\n",
    "            elif len(context_dict[current_context][\"rewards\"]) == 0:  # check if participant has been able to make any observations in this context yet \n",
    "                # if not then let choice be random, and store observations into context dict\n",
    "                options = [op1[i], op2[i]]\n",
    "                choice = choices[i]\n",
    "                reward = rewards[i]\n",
    "\n",
    "                context_dict[current_context][\"training_idx\"].append(choice)\n",
    "                context_dict[current_context][\"rewards\"].append(reward)\n",
    "                context_dict[current_context][\"state_rewards\"][choice] = reward\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                options = [op1[i], op2[i]]\n",
    "                choice = choices[i]\n",
    "                reward = rewards[i]\n",
    "\n",
    "                training_idx = context_dict[current_context][\"training_idx\"] # the training indices for the gps\n",
    "                R = copy.copy(context_dict[current_context][\"state_rewards\"]) # an array with rewards for each state for the SR. We copy so that it doesn't change when we normalize it\n",
    "\n",
    "                if R.std() != 0:\n",
    "                    R = (R - R.mean())/R.std()\n",
    "                else:\n",
    "                    R = (R - R.mean())\n",
    "\n",
    "                y = np.array(copy.copy(context_dict[current_context][\"rewards\"]))  # for use in the gp models. we copy this so we can normalize it and convert it into an array without messing with the original set of reward observations\n",
    "                if y.std() != 0:\n",
    "                    y = (y- y.mean())/y.std()\n",
    "\n",
    "                else:\n",
    "                    y = (y - y.mean())\n",
    "\n",
    "                ### sr softmax #####\n",
    "\n",
    "                if model == \"SR-softmax\":\n",
    "\n",
    "                    SR_K = SR_dict[current_context][\"kernel\"]\n",
    "                    T_prior = SR_dict[current_context][\"T\"]\n",
    "\n",
    "                    K = (SR_K + estimated_euclidean_K)/2\n",
    "                    rewards_est = estimate_GP_full(K, y, training_idx)\n",
    "                    diff = rewards_est[options[0]] - rewards_est[options[1]]\n",
    "\n",
    "                    L_new, T_new = SR_softmax(T_prior, rewards_est)\n",
    "                    SR_K = scipy.linalg.expm(-1*L_new)\n",
    "                    SR_dict[current_context][\"kernel\"] = SR_K\n",
    "                    SR_dict[current_context][\"T\"] = T_new\n",
    "                    T_prior = T_new\n",
    "\n",
    "                elif model == \"Euclidean-optimized\":\n",
    "\n",
    "                    ## optimized gp\n",
    "                    diff = optimize_gp(monster_loc, training_idx, y, option_indices = options)\n",
    "                elif model == \"SR\":\n",
    "                    ##SR\n",
    "                    diff = estimate_successor_model(SR, R, option_indices=options)\n",
    "\n",
    "                elif model == \"Euclidean\":\n",
    "\n",
    "                    ### Euclidean\n",
    "                    diff = estimate_euclidean_model(estimated_euclidean_K, y, training_idx, option_indices=options)\n",
    "\n",
    "                elif model == \"Temporal\":\n",
    "                    ## SR graph\n",
    "                    diff = estimate_GP(SR_K, y, training_idx, option_indices=options)\n",
    "\n",
    "                elif model == \"Compositional\":\n",
    "                    # compositional model\n",
    "                    diff = estimate_GP(compositional_kernel, y, training_idx, option_indices=options)\n",
    "\n",
    "\n",
    "                ### update arrays:\n",
    "                context_dict[current_context][\"training_idx\"].append(choice)\n",
    "                context_dict[current_context][\"rewards\"].append(reward)\n",
    "                context_dict[current_context][\"state_rewards\"][choice] = reward\n",
    "\n",
    "                ## store data\n",
    "                data_frame[i, n] = diff\n",
    "    data_frame = pd.DataFrame(data_frame)\n",
    "    data_frame.to_csv(f\"param_fits/{file_name}\", header=header, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress %:  0.0\n",
      "progress %:  0.0025\n",
      "progress %:  0.005\n",
      "progress %:  0.0075\n",
      "progress %:  0.01\n",
      "progress %:  0.0125\n",
      "progress %:  0.015\n",
      "progress %:  0.0175\n",
      "progress %:  0.02\n",
      "progress %:  0.0225\n",
      "progress %:  0.025\n",
      "progress %:  0.0275\n",
      "progress %:  0.03\n",
      "progress %:  0.0325\n",
      "progress %:  0.035\n",
      "progress %:  0.0375\n",
      "progress %:  0.04\n",
      "progress %:  0.0425\n",
      "progress %:  0.045\n",
      "progress %:  0.0475\n",
      "progress %:  0.05\n",
      "progress %:  0.0525\n",
      "progress %:  0.055\n",
      "progress %:  0.0575\n",
      "progress %:  0.06\n",
      "progress %:  0.0625\n",
      "progress %:  0.065\n",
      "progress %:  0.0675\n",
      "progress %:  0.07\n",
      "progress %:  0.0725\n",
      "progress %:  0.075\n",
      "progress %:  0.0775\n",
      "progress %:  0.08\n",
      "progress %:  0.0825\n",
      "progress %:  0.085\n",
      "progress %:  0.0875\n",
      "progress %:  0.09\n",
      "progress %:  0.0925\n",
      "progress %:  0.095\n",
      "progress %:  0.0975\n",
      "progress %:  0.1\n",
      "progress %:  0.1025\n",
      "progress %:  0.105\n",
      "progress %:  0.1075\n",
      "progress %:  0.11\n",
      "progress %:  0.1125\n",
      "progress %:  0.115\n",
      "progress %:  0.1175\n",
      "progress %:  0.12\n",
      "progress %:  0.1225\n",
      "progress %:  0.125\n",
      "progress %:  0.1275\n",
      "progress %:  0.13\n",
      "progress %:  0.1325\n",
      "progress %:  0.135\n",
      "progress %:  0.1375\n",
      "progress %:  0.14\n",
      "progress %:  0.1425\n",
      "progress %:  0.145\n",
      "progress %:  0.1475\n",
      "progress %:  0.15\n",
      "progress %:  0.1525\n",
      "progress %:  0.155\n",
      "progress %:  0.1575\n",
      "progress %:  0.16\n",
      "progress %:  0.1625\n",
      "progress %:  0.165\n",
      "progress %:  0.1675\n",
      "progress %:  0.17\n",
      "progress %:  0.1725\n",
      "progress %:  0.175\n",
      "progress %:  0.1775\n",
      "progress %:  0.18\n",
      "progress %:  0.1825\n",
      "progress %:  0.185\n",
      "progress %:  0.1875\n",
      "progress %:  0.19\n",
      "progress %:  0.1925\n",
      "progress %:  0.195\n",
      "progress %:  0.1975\n",
      "progress %:  0.2\n",
      "progress %:  0.2025\n",
      "progress %:  0.205\n",
      "progress %:  0.2075\n",
      "progress %:  0.21\n",
      "progress %:  0.2125\n",
      "progress %:  0.215\n",
      "progress %:  0.2175\n",
      "progress %:  0.22\n",
      "progress %:  0.2225\n",
      "progress %:  0.225\n",
      "progress %:  0.2275\n",
      "progress %:  0.23\n",
      "progress %:  0.2325\n",
      "progress %:  0.235\n",
      "progress %:  0.2375\n",
      "progress %:  0.24\n",
      "progress %:  0.2425\n",
      "progress %:  0.245\n",
      "progress %:  0.2475\n",
      "progress %:  0.25\n",
      "progress %:  0.2525\n",
      "progress %:  0.255\n",
      "progress %:  0.2575\n",
      "progress %:  0.26\n",
      "progress %:  0.2625\n",
      "progress %:  0.265\n",
      "progress %:  0.2675\n",
      "progress %:  0.27\n",
      "progress %:  0.2725\n",
      "progress %:  0.275\n",
      "progress %:  0.2775\n",
      "progress %:  0.28\n",
      "progress %:  0.2825\n",
      "progress %:  0.285\n",
      "progress %:  0.2875\n",
      "progress %:  0.29\n",
      "progress %:  0.2925\n",
      "progress %:  0.295\n",
      "progress %:  0.2975\n",
      "progress %:  0.3\n",
      "progress %:  0.3025\n",
      "progress %:  0.305\n",
      "progress %:  0.3075\n",
      "progress %:  0.31\n",
      "progress %:  0.3125\n",
      "progress %:  0.315\n",
      "progress %:  0.3175\n",
      "progress %:  0.32\n",
      "progress %:  0.3225\n",
      "progress %:  0.325\n",
      "progress %:  0.3275\n",
      "progress %:  0.33\n",
      "progress %:  0.3325\n",
      "progress %:  0.335\n",
      "progress %:  0.3375\n",
      "progress %:  0.34\n",
      "progress %:  0.3425\n",
      "progress %:  0.345\n",
      "progress %:  0.3475\n",
      "progress %:  0.35\n",
      "progress %:  0.3525\n",
      "progress %:  0.355\n",
      "progress %:  0.3575\n",
      "progress %:  0.36\n",
      "progress %:  0.3625\n",
      "progress %:  0.365\n",
      "progress %:  0.3675\n",
      "progress %:  0.37\n",
      "progress %:  0.3725\n",
      "progress %:  0.375\n",
      "progress %:  0.3775\n",
      "progress %:  0.38\n",
      "progress %:  0.3825\n",
      "progress %:  0.385\n",
      "progress %:  0.3875\n",
      "progress %:  0.39\n",
      "progress %:  0.3925\n",
      "progress %:  0.395\n",
      "progress %:  0.3975\n",
      "progress %:  0.4\n",
      "progress %:  0.4025\n",
      "progress %:  0.405\n",
      "progress %:  0.4075\n",
      "progress %:  0.41\n",
      "progress %:  0.4125\n",
      "progress %:  0.415\n",
      "progress %:  0.4175\n",
      "progress %:  0.42\n",
      "progress %:  0.4225\n",
      "progress %:  0.425\n",
      "progress %:  0.4275\n",
      "progress %:  0.43\n",
      "progress %:  0.4325\n",
      "progress %:  0.435\n",
      "progress %:  0.4375\n",
      "progress %:  0.44\n",
      "progress %:  0.4425\n",
      "progress %:  0.445\n",
      "progress %:  0.4475\n",
      "progress %:  0.45\n",
      "progress %:  0.4525\n",
      "progress %:  0.455\n",
      "progress %:  0.4575\n",
      "progress %:  0.46\n",
      "progress %:  0.4625\n",
      "progress %:  0.465\n",
      "progress %:  0.4675\n",
      "progress %:  0.47\n",
      "progress %:  0.4725\n",
      "progress %:  0.475\n",
      "progress %:  0.4775\n",
      "progress %:  0.48\n",
      "progress %:  0.4825\n",
      "progress %:  0.485\n",
      "progress %:  0.4875\n",
      "progress %:  0.49\n",
      "progress %:  0.4925\n",
      "progress %:  0.495\n",
      "progress %:  0.4975\n",
      "progress %:  0.5\n",
      "progress %:  0.5025\n",
      "progress %:  0.505\n",
      "progress %:  0.5075\n",
      "progress %:  0.51\n",
      "progress %:  0.5125\n",
      "progress %:  0.515\n",
      "progress %:  0.5175\n",
      "progress %:  0.52\n",
      "progress %:  0.5225\n",
      "progress %:  0.525\n",
      "progress %:  0.5275\n",
      "progress %:  0.53\n",
      "progress %:  0.5325\n",
      "progress %:  0.535\n",
      "progress %:  0.5375\n",
      "progress %:  0.54\n",
      "progress %:  0.5425\n",
      "progress %:  0.545\n",
      "progress %:  0.5475\n",
      "progress %:  0.55\n",
      "progress %:  0.5525\n",
      "progress %:  0.555\n",
      "progress %:  0.5575\n",
      "progress %:  0.56\n",
      "progress %:  0.5625\n",
      "progress %:  0.565\n",
      "progress %:  0.5675\n",
      "progress %:  0.57\n",
      "progress %:  0.5725\n",
      "progress %:  0.575\n",
      "progress %:  0.5775\n",
      "progress %:  0.58\n",
      "progress %:  0.5825\n",
      "progress %:  0.585\n",
      "progress %:  0.5875\n",
      "progress %:  0.59\n",
      "progress %:  0.5925\n",
      "progress %:  0.595\n",
      "progress %:  0.5975\n",
      "progress %:  0.6\n",
      "progress %:  0.6025\n",
      "progress %:  0.605\n",
      "progress %:  0.6075\n",
      "progress %:  0.61\n",
      "progress %:  0.6125\n",
      "progress %:  0.615\n",
      "progress %:  0.6175\n",
      "progress %:  0.62\n",
      "progress %:  0.6225\n",
      "progress %:  0.625\n",
      "progress %:  0.6275\n",
      "progress %:  0.63\n",
      "progress %:  0.6325\n",
      "progress %:  0.635\n",
      "progress %:  0.6375\n",
      "progress %:  0.64\n",
      "progress %:  0.6425\n",
      "progress %:  0.645\n",
      "progress %:  0.6475\n",
      "progress %:  0.65\n",
      "progress %:  0.6525\n",
      "progress %:  0.655\n",
      "progress %:  0.6575\n",
      "progress %:  0.66\n",
      "progress %:  0.6625\n",
      "progress %:  0.665\n",
      "progress %:  0.6675\n",
      "progress %:  0.67\n",
      "progress %:  0.6725\n",
      "progress %:  0.675\n",
      "progress %:  0.6775\n",
      "progress %:  0.68\n",
      "progress %:  0.6825\n",
      "progress %:  0.685\n",
      "progress %:  0.6875\n",
      "progress %:  0.69\n",
      "progress %:  0.6925\n",
      "progress %:  0.695\n",
      "progress %:  0.6975\n",
      "progress %:  0.7\n",
      "progress %:  0.7025\n",
      "progress %:  0.705\n",
      "progress %:  0.7075\n",
      "progress %:  0.71\n",
      "progress %:  0.7125\n",
      "progress %:  0.715\n",
      "progress %:  0.7175\n",
      "progress %:  0.72\n",
      "progress %:  0.7225\n",
      "progress %:  0.725\n",
      "progress %:  0.7275\n",
      "progress %:  0.73\n",
      "progress %:  0.7325\n",
      "progress %:  0.735\n",
      "progress %:  0.7375\n",
      "progress %:  0.74\n",
      "progress %:  0.7425\n",
      "progress %:  0.745\n",
      "progress %:  0.7475\n",
      "progress %:  0.75\n",
      "progress %:  0.7525\n",
      "progress %:  0.755\n",
      "progress %:  0.7575\n",
      "progress %:  0.76\n",
      "progress %:  0.7625\n",
      "progress %:  0.765\n",
      "progress %:  0.7675\n",
      "progress %:  0.77\n",
      "progress %:  0.7725\n",
      "progress %:  0.775\n",
      "progress %:  0.7775\n",
      "progress %:  0.78\n",
      "progress %:  0.7825\n",
      "progress %:  0.785\n",
      "progress %:  0.7875\n",
      "progress %:  0.79\n",
      "progress %:  0.7925\n",
      "progress %:  0.795\n",
      "progress %:  0.7975\n",
      "progress %:  0.8\n",
      "progress %:  0.8025\n",
      "progress %:  0.805\n",
      "progress %:  0.8075\n",
      "progress %:  0.81\n",
      "progress %:  0.8125\n",
      "progress %:  0.815\n",
      "progress %:  0.8175\n",
      "progress %:  0.82\n",
      "progress %:  0.8225\n",
      "progress %:  0.825\n",
      "progress %:  0.8275\n",
      "progress %:  0.83\n",
      "progress %:  0.8325\n",
      "progress %:  0.835\n",
      "progress %:  0.8375\n",
      "progress %:  0.84\n",
      "progress %:  0.8425\n",
      "progress %:  0.845\n",
      "progress %:  0.8475\n",
      "progress %:  0.85\n",
      "progress %:  0.8525\n",
      "progress %:  0.855\n",
      "progress %:  0.8575\n",
      "progress %:  0.86\n",
      "progress %:  0.8625\n",
      "progress %:  0.865\n",
      "progress %:  0.8675\n",
      "progress %:  0.87\n",
      "progress %:  0.8725\n",
      "progress %:  0.875\n",
      "progress %:  0.8775\n",
      "progress %:  0.88\n",
      "progress %:  0.8825\n",
      "progress %:  0.885\n",
      "progress %:  0.8875\n",
      "progress %:  0.89\n",
      "progress %:  0.8925\n",
      "progress %:  0.895\n",
      "progress %:  0.8975\n",
      "progress %:  0.9\n",
      "progress %:  0.9025\n",
      "progress %:  0.905\n",
      "progress %:  0.9075\n",
      "progress %:  0.91\n",
      "progress %:  0.9125\n",
      "progress %:  0.915\n",
      "progress %:  0.9175\n",
      "progress %:  0.92\n",
      "progress %:  0.9225\n",
      "progress %:  0.925\n",
      "progress %:  0.9275\n",
      "progress %:  0.93\n",
      "progress %:  0.9325\n",
      "progress %:  0.935\n",
      "progress %:  0.9375\n",
      "progress %:  0.94\n",
      "progress %:  0.9425\n",
      "progress %:  0.945\n",
      "progress %:  0.9475\n",
      "progress %:  0.95\n",
      "progress %:  0.9525\n",
      "progress %:  0.955\n",
      "progress %:  0.9575\n",
      "progress %:  0.96\n",
      "progress %:  0.9625\n",
      "progress %:  0.965\n",
      "progress %:  0.9675\n",
      "progress %:  0.97\n",
      "progress %:  0.9725\n",
      "progress %:  0.975\n",
      "progress %:  0.9775\n",
      "progress %:  0.98\n",
      "progress %:  0.9825\n",
      "progress %:  0.985\n",
      "progress %:  0.9875\n",
      "progress %:  0.99\n",
      "progress %:  0.9925\n",
      "progress %:  0.995\n",
      "progress %:  0.9975\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### define hyperparamater search grids and run the search loop\n",
    "### remember we have the following model strings to input:\n",
    "##    - \"Euclidean\"\n",
    "##    - \"Temporal\"\n",
    "##    - \"SR\"\n",
    "##    - \"Compositional\"\n",
    "##    - \"Euclidean-optimized\"\n",
    "##    - \"SR-softmax\"\n",
    "\n",
    "\n",
    "## Euclidean\n",
    "\n",
    "# euc_num_samples = 100\n",
    "# euc_params = np.linspace(0, 99, euc_num_samples, dtype=int)\n",
    "# # these are indices for the estimated euclidean kernels with various lengthscale settings,\n",
    "# # estimated individually for each subject. See \"compute_euclidean_graphs.py\"\n",
    "\n",
    "# param_search(params=euc_params, model=\"Euclidean\", header_is_params=True, file_name=\"euc_results.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Temporal\n",
    "# temporal_num_samples = 25  # we use less samples here because the search space is a lot smaller\n",
    "# temporal_params = np.linspace(0.01, 0.7, temporal_num_samples)\n",
    "\n",
    "# param_search(params=temporal_params, model=\"Temporal\", header_is_params=True, file_name=\"sr_gp_results_v3_final.csv\")\n",
    "\n",
    "## SR\n",
    "# sr_num_samples = 20\n",
    "# sr_params = np.linspace(0.01, 0.7, sr_num_samples)\n",
    "\n",
    "# param_search(params=sr_params, model=\"SR\", header_is_params=True, file_name=\"sr_results.csv\")\n",
    "\n",
    "## Compositional\n",
    "## here the parameter space is 2D, so the number of samples scales quadratically with\n",
    "## the number of samples in the 1D case. Therefore we restrict ourselves to fewer\n",
    "## samples in the single dimensions.\n",
    "\n",
    "comp_num_samples1D = 20 \n",
    "comp_l = np.linspace(0, 99, comp_num_samples1D, dtype=int)\n",
    "comp_lr = np.linspace(0.01, 0.7, comp_num_samples1D)\n",
    "xi, yi = np.meshgrid(comp_lr, comp_l)\n",
    "comp_params = np.zeros((len(xi.ravel()), 2))\n",
    "comp_params[:, 0] = xi.ravel()\n",
    "comp_params[:, 1] = yi.ravel()\n",
    "comp_num_samples = comp_num_samples1D * comp_num_samples1D\n",
    "\n",
    "param_search(params=comp_params, model=\"Compositional\", header_is_params=False, file_name=\"comp_results_v3_final.csv\")\n",
    "\n",
    "## Optimized Euclidean\n",
    "## this doesnt use any hyperparameters\n",
    "# op_num_samples = 1\n",
    "# op_params = [1]\n",
    "\n",
    "# param_search(params=op_params, model=\"Euclidean-optimized\", header_is_params=True, file_name=\"optimized_results.csv\")\n",
    "\n",
    "## SR softmax\n",
    "## same case as with the compositional model.\n",
    "\n",
    "# softmax_num_samples1D = 20\n",
    "# softmax_l = np.linspace(0, 99, softmax_num_samples1D, dtype=int)\n",
    "# softmax_lr = np.linspace(0.01, 0.7, comp_num_samples1D)\n",
    "# xi, yi = np.meshgrid(softmax_lr, softmax_l)\n",
    "# softmax_params = np.zeros((len(xi.ravel()), 2))\n",
    "# softmax_params[:, 0] = xi.ravel()\n",
    "# softmax_params[:, 1] = yi.ravel()\n",
    "# softmax_num_samples = softmax_num_samples1D * softmax_num_samples1D\n",
    "\n",
    "# param_search(params=softmax_params, model=\"SR-softmax\", header_is_params=False, file_name=\"softmax_sr.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
